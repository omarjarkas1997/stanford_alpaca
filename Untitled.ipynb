{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb85e77-ba85-48c5-bfa5-0ab97cbd1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 24.2 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n",
      "Requirement already satisfied: openai==0.27.0 in /usr/local/lib/python3.8/dist-packages (0.27.0)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.8/dist-packages (0.1.2)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai==0.27.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai==0.27.0) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai==0.27.0) (3.10.10)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0) (4.0.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->rouge-score) (2024.9.11)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.0) (4.12.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.27.0) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mUsing pip 24.2 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Collecting fire (from -r requirements.txt (line 3))\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Running command python setup.py egg_info\n",
      "  running egg_info\n",
      "  creating /tmp/pip-pip-egg-info-6tev_jou/fire.egg-info\n",
      "  writing /tmp/pip-pip-egg-info-6tev_jou/fire.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-pip-egg-info-6tev_jou/fire.egg-info/dependency_links.txt\n",
      "  writing requirements to /tmp/pip-pip-egg-info-6tev_jou/fire.egg-info/requires.txt\n",
      "  writing top-level names to /tmp/pip-pip-egg-info-6tev_jou/fire.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-6tev_jou/fire.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-pip-egg-info-6tev_jou/fire.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  writing manifest file '/tmp/pip-pip-egg-info-6tev_jou/fire.egg-info/SOURCES.txt'\n",
      "  /usr/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'requires_python'\n",
      "    warnings.warn(msg)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: openai==0.27.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (0.27.0)\n",
      "Collecting transformers>=4.28.1 (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for transformers>=4.28.1 from https://files.pythonhosted.org/packages/f9/9d/030cc1b3e88172967e22ee1d012e0d5e0384eb70d2a098d1669d549aea29/transformers-4.45.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/a9/71/45aac46b75742e08d2d6f9fc2b612223b5e36115b8b2ed673b23c21b5387/torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 7))\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/d4/eb/57f1f43f60aa3a21296171d353b6597c312b45d9a5addb1fb5313ec3611a/sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tokenizers>=0.13.3 (from -r requirements.txt (line 8))\n",
      "  Obtaining dependency information for tokenizers>=0.13.3 from https://files.pythonhosted.org/packages/15/e8/67dc15b6ed227358476fcfa6c2c71e4c508ff86cb6a5996c72e316ebac5b/tokenizers-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting wandb (from -r requirements.txt (line 9))\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/e2/ae/943335dd9b8783db59133047a1fdf53f5070cadab7ce2464632e36ba6b7f/wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai==0.27.0->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai==0.27.0->-r requirements.txt (line 4)) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai==0.27.0->-r requirements.txt (line 4)) (3.10.10)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge_score->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from rouge_score->-r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/7f/be/df630c387a0a054815d60be6a97eb4e8f17385d5d6fe660e1c02750062b4/termcolor-2.5.0-py3-none-any.whl (from https://pypi.org/simple/termcolor/) (requires-python:>=3.9)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/37/72/88311445fd44c455c7d553e61f95412cf89054308a1aa2434ab835075fc5/termcolor-2.5.0.tar.gz (from https://pypi.org/simple/termcolor/) (requires-python:>=3.9)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for termcolor from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from transformers>=4.28.1->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers>=4.28.1->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.23.2 from https://files.pythonhosted.org/packages/64/09/a535946bf2dc88e61341f39dc507530411bb3ea4eac493e5ec833e8f35bd/huggingface_hub-0.25.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (2024.9.11)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.28.1->-r requirements.txt (line 5))\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/53/62/1d6ffba0a2bc0e6b9f5b50d421493276d9fac5ef49670d06f7b66ea73500/safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 6)) (4.12.2)\n",
      "Collecting sympy (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for sympy from https://files.pythonhosted.org/packages/99/ff/c87e0622b1dadea79d2fb0b25ade9ed98954c9033722eb707053d310d4f3/sympy-1.13.3-py3-none-any.whl.metadata\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/49/38/8433925423d52727d286e1e7d6a234e8f2f084cf84a1942b122231c6ef76/networkx-3.2rc0-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.9)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/f1/94/5e42e01e816a797801348b773f045e1d7650a104d44f89a39430188fa105/networkx-3.2rc0.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.9)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/f6/eb/5585c96636bbb2755865c31d83a19dd220ef88e716df4659dacb86e009cc/networkx-3.2-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.9)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/86/36/f9367f30c63a9e10f387783dc18b2e0454dc9debb25151b7a6d592590375/networkx-3.2.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.9)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/d5/f0/8fbc882ca80cf077f1b246c0e3c3465f7f415439bdea6b899f6b19f61f70/networkx-3.2.1-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.9)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.9'): https://files.pythonhosted.org/packages/c4/80/a84676339aaae2f1cfdf9f418701dd634aef9cc76f708ef55c36ff39c3ca/networkx-3.2.1.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.9)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/35/2f/af61ca2d0a6679fda4066f8b22812aa8c6ac16abfacb59e337cf4c9abfe3/networkx-3.3rc0-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/d5/e1/104908a8059dd88fb7338a7fc05d3a513491384abcc188ffb507b2e53f92/networkx-3.3rc0.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/38/e9/5f72929373e1a0e8d142a130f3f97e6ff920070f87f91c4e13e40e0fba5a/networkx-3.3-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/04/e6/b164f94c869d6b2c605b5128b7b0cfe912795a87fc90e78533920001f3ec/networkx-3.3.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/39/d5/1b381a4413a89801866bc78cca4620e0cf8712c552ddca10feb6516c45b2/networkx-3.4rc0-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/e2/3b/0e2d8974b340fee65c0d64cfab53bd43a85854d471edfb2a6a821970956e/networkx-3.4rc0.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/fd/81/fc591ae6a55b34bcffde396fc29769095148711ae19b9932e139442c8a37/networkx-3.4-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/04/e7/2861289b4545178e2de30789c2ac9c260e334d3555837a959225e6909b86/networkx-3.4.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/8b/4e/bf7a4ccc11ded738efd0bda39296c7cee3617e800f890f919de5c0fe00c8/networkx-3.4.1-py3-none-any.whl (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "  Link requires a different Python (3.8.10 not in: '>=3.10'): https://files.pythonhosted.org/packages/36/2b/20ad9eecdda3f1b0dc63fb8f82d2ea99163dbca08bfa392594fc2ed81869/networkx-3.4.1.tar.gz (from https://pypi.org/simple/networkx/) (requires-python:>=3.10)\n",
      "Collecting networkx (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/a8/05/9d4f9b78ead6b2661d6e8ea772e111fc4a9fbd866ad0c81906c11206b55e/networkx-3.1-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.4)\n",
      "Collecting fsspec (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.1.105 from https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.1.105 from https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.1.105 from https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==9.1.0.70 from https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cublas-cu12==12.1.3.1 from https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cufft-cu12==11.0.2.54 from https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.2.106 from https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cusolver-cu12==11.4.5.107 from https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-cusparse-cu12==12.1.0.106 from https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-nccl-cu12==2.20.5 from https://files.pythonhosted.org/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-nvtx-cu12==12.1.105 from https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for triton==3.0.0 from https://files.pythonhosted.org/packages/4d/b4/c37e2776a1390bab7e78a6d52bd525441cb3cad7260a6a00b11b0b702e7c/triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/fe/e4/486de766851d58699bcfeb3ba6a3beb4d89c3809f75b9d423b9508a8760f/nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 9)) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Obtaining dependency information for docker-pycreds>=0.4.0 from https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Obtaining dependency information for gitpython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/e9/bd/cc3a402a6439c15c3d4294333e13042b915bbeab54edc457c723931fed3f/GitPython-3.1.43-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 9)) (4.3.6)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.12.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=5.28.0,<6,>=3.12.0 from https://files.pythonhosted.org/packages/23/08/a1ce0415a115c2b703bfa798f06f0e43ca91dbe29d6180bf86a9287b15e2/protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb->-r requirements.txt (line 9)) (6.1.0)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb->-r requirements.txt (line 9))\n",
      "  Obtaining dependency information for sentry-sdk>=2.0.0 from https://files.pythonhosted.org/packages/10/63/8e80fff3aa15488bc332ede44165a397a29bb13ec4a4b2236299e3b66067/sentry_sdk-2.17.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting setproctitle (from wandb->-r requirements.txt (line 9))\n",
      "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/45/8d/68eec8de2d22a8ed6004344b35f94f2407ba723beee6ab468f162bb7be3e/setproctitle-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading setproctitle-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb->-r requirements.txt (line 9)) (45.2.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0->-r requirements.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.0->-r requirements.txt (line 4)) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.4.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.27.0->-r requirements.txt (line 4)) (0.2.0)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl (314 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Running command python setup.py bdist_wheel\n",
      "  /usr/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'requires_python'\n",
      "    warnings.warn(msg)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  creating build/lib/fire\n",
      "  copying fire/decorators.py -> build/lib/fire\n",
      "  copying fire/interact.py -> build/lib/fire\n",
      "  copying fire/completion_test.py -> build/lib/fire\n",
      "  copying fire/docstrings_test.py -> build/lib/fire\n",
      "  copying fire/docstrings_fuzz_test.py -> build/lib/fire\n",
      "  copying fire/test_components_bin.py -> build/lib/fire\n",
      "  copying fire/decorators_test.py -> build/lib/fire\n",
      "  copying fire/test_components.py -> build/lib/fire\n",
      "  copying fire/formatting_test.py -> build/lib/fire\n",
      "  copying fire/core.py -> build/lib/fire\n",
      "  copying fire/parser_fuzz_test.py -> build/lib/fire\n",
      "  copying fire/testutils_test.py -> build/lib/fire\n",
      "  copying fire/value_types.py -> build/lib/fire\n",
      "  copying fire/test_components_test.py -> build/lib/fire\n",
      "  copying fire/main_test.py -> build/lib/fire\n",
      "  copying fire/parser.py -> build/lib/fire\n",
      "  copying fire/helptext.py -> build/lib/fire\n",
      "  copying fire/inspectutils.py -> build/lib/fire\n",
      "  copying fire/custom_descriptions.py -> build/lib/fire\n",
      "  copying fire/testutils.py -> build/lib/fire\n",
      "  copying fire/fire_test.py -> build/lib/fire\n",
      "  copying fire/completion.py -> build/lib/fire\n",
      "  copying fire/fire_import_test.py -> build/lib/fire\n",
      "  copying fire/trace.py -> build/lib/fire\n",
      "  copying fire/formatting.py -> build/lib/fire\n",
      "  copying fire/formatting_windows.py -> build/lib/fire\n",
      "  copying fire/helptext_test.py -> build/lib/fire\n",
      "  copying fire/test_components_py3.py -> build/lib/fire\n",
      "  copying fire/core_test.py -> build/lib/fire\n",
      "  copying fire/__main__.py -> build/lib/fire\n",
      "  copying fire/trace_test.py -> build/lib/fire\n",
      "  copying fire/inspectutils_test.py -> build/lib/fire\n",
      "  copying fire/__init__.py -> build/lib/fire\n",
      "  copying fire/docstrings.py -> build/lib/fire\n",
      "  copying fire/parser_test.py -> build/lib/fire\n",
      "  copying fire/interact_test.py -> build/lib/fire\n",
      "  copying fire/custom_descriptions_test.py -> build/lib/fire\n",
      "  creating build/lib/fire/console\n",
      "  copying fire/console/files.py -> build/lib/fire/console\n",
      "  copying fire/console/platforms.py -> build/lib/fire/console\n",
      "  copying fire/console/encoding.py -> build/lib/fire/console\n",
      "  copying fire/console/text.py -> build/lib/fire/console\n",
      "  copying fire/console/console_io.py -> build/lib/fire/console\n",
      "  copying fire/console/console_attr_os.py -> build/lib/fire/console\n",
      "  copying fire/console/console_pager.py -> build/lib/fire/console\n",
      "  copying fire/console/console_attr.py -> build/lib/fire/console\n",
      "  copying fire/console/__init__.py -> build/lib/fire/console\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/decorators.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/interact.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/completion_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/docstrings_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/docstrings_fuzz_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/test_components_bin.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/decorators_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/test_components.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/formatting_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/core.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/parser_fuzz_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/testutils_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/value_types.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/test_components_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/main_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/parser.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/helptext.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/inspectutils.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/custom_descriptions.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/testutils.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/fire_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/completion.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/fire_import_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/trace.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/formatting.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/formatting_windows.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/helptext_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/test_components_py3.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/core_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/__main__.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/trace_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/inspectutils_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/__init__.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/docstrings.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/parser_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/interact_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  copying build/lib/fire/custom_descriptions_test.py -> build/bdist.linux-x86_64/wheel/fire\n",
      "  creating build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/files.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/platforms.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/encoding.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/text.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/console_io.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/console_attr_os.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/console_pager.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/console_attr.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  copying build/lib/fire/console/__init__.py -> build/bdist.linux-x86_64/wheel/fire/console\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing fire.egg-info/PKG-INFO\n",
      "  writing dependency_links to fire.egg-info/dependency_links.txt\n",
      "  writing requirements to fire.egg-info/requires.txt\n",
      "  writing top-level names to fire.egg-info/top_level.txt\n",
      "  reading manifest file 'fire.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  writing manifest file 'fire.egg-info/SOURCES.txt'\n",
      "  Copying fire.egg-info to build/bdist.linux-x86_64/wheel/fire-0.7.0.egg-info\n",
      "  running install_scripts\n",
      "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
      "  creating build/bdist.linux-x86_64/wheel/fire-0.7.0.dist-info/WHEEL\n",
      "  creating '/tmp/pip-wheel-w2zyebv4/fire-0.7.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "  adding 'fire/__init__.py'\n",
      "  adding 'fire/__main__.py'\n",
      "  adding 'fire/completion.py'\n",
      "  adding 'fire/completion_test.py'\n",
      "  adding 'fire/core.py'\n",
      "  adding 'fire/core_test.py'\n",
      "  adding 'fire/custom_descriptions.py'\n",
      "  adding 'fire/custom_descriptions_test.py'\n",
      "  adding 'fire/decorators.py'\n",
      "  adding 'fire/decorators_test.py'\n",
      "  adding 'fire/docstrings.py'\n",
      "  adding 'fire/docstrings_fuzz_test.py'\n",
      "  adding 'fire/docstrings_test.py'\n",
      "  adding 'fire/fire_import_test.py'\n",
      "  adding 'fire/fire_test.py'\n",
      "  adding 'fire/formatting.py'\n",
      "  adding 'fire/formatting_test.py'\n",
      "  adding 'fire/formatting_windows.py'\n",
      "  adding 'fire/helptext.py'\n",
      "  adding 'fire/helptext_test.py'\n",
      "  adding 'fire/inspectutils.py'\n",
      "  adding 'fire/inspectutils_test.py'\n",
      "  adding 'fire/interact.py'\n",
      "  adding 'fire/interact_test.py'\n",
      "  adding 'fire/main_test.py'\n",
      "  adding 'fire/parser.py'\n",
      "  adding 'fire/parser_fuzz_test.py'\n",
      "  adding 'fire/parser_test.py'\n",
      "  adding 'fire/test_components.py'\n",
      "  adding 'fire/test_components_bin.py'\n",
      "  adding 'fire/test_components_py3.py'\n",
      "  adding 'fire/test_components_test.py'\n",
      "  adding 'fire/testutils.py'\n",
      "  adding 'fire/testutils_test.py'\n",
      "  adding 'fire/trace.py'\n",
      "  adding 'fire/trace_test.py'\n",
      "  adding 'fire/value_types.py'\n",
      "  adding 'fire/console/__init__.py'\n",
      "  adding 'fire/console/console_attr.py'\n",
      "  adding 'fire/console/console_attr_os.py'\n",
      "  adding 'fire/console/console_io.py'\n",
      "  adding 'fire/console/console_pager.py'\n",
      "  adding 'fire/console/encoding.py'\n",
      "  adding 'fire/console/files.py'\n",
      "  adding 'fire/console/platforms.py'\n",
      "  adding 'fire/console/text.py'\n",
      "  adding 'fire-0.7.0.dist-info/LICENSE'\n",
      "  adding 'fire-0.7.0.dist-info/METADATA'\n",
      "  adding 'fire-0.7.0.dist-info/WHEEL'\n",
      "  adding 'fire-0.7.0.dist-info/top_level.txt'\n",
      "  adding 'fire-0.7.0.dist-info/RECORD'\n",
      "  removing build/bdist.linux-x86_64/wheel\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=fe8b77e3fca91cce408b1483c70074ee7670db4936dc6e6fd15b8e53bf35f330\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ywi7gkwg/wheels/e4/c0/6a/3b7584a4fa718f4d38126c56287b00665949885a3aaea27235\n",
      "Successfully built fire\n",
      "Installing collected packages: sentencepiece, mpmath, termcolor, sympy, smmap, setproctitle, sentry-sdk, safetensors, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, docker-pycreds, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, gitdb, fire, tokenizers, nvidia-cusolver-cu12, gitpython, wandb, transformers, torch\n",
      "  changing mode of /usr/local/bin/isympy to 755\n",
      "  changing mode of /usr/local/bin/proton to 755\n",
      "  changing mode of /usr/local/bin/proton-viewer to 755\n",
      "  changing mode of /usr/local/bin/huggingface-cli to 755\n",
      "  changing mode of /usr/local/bin/wandb to 755\n",
      "  changing mode of /usr/local/bin/wb to 755\n",
      "  changing mode of /usr/local/bin/transformers-cli to 755\n",
      "  changing mode of /usr/local/bin/convert-caffe2-to-onnx to 755\n",
      "  changing mode of /usr/local/bin/convert-onnx-to-caffe2 to 755\n",
      "  changing mode of /usr/local/bin/torchrun to 755\n",
      "Successfully installed docker-pycreds-0.4.0 filelock-3.16.1 fire-0.7.0 fsspec-2024.9.0 gitdb-4.0.11 gitpython-3.1.43 huggingface-hub-0.25.2 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 protobuf-5.28.2 safetensors-0.4.5 sentencepiece-0.2.0 sentry-sdk-2.17.0 setproctitle-1.3.3 smmap-5.0.1 sympy-1.13.3 termcolor-2.4.0 tokenizers-0.20.1 torch-2.4.1 transformers-4.45.2 triton-3.0.0 wandb-0.18.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --no-cache-dir -v openai==0.27.0 rouge-score\n",
    "!pip3 install --no-cache-dir -v -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "96709907-6e95-42af-b393-34347e12affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"seed_task_0\",\n",
      "    \"name\": \"breakfast_suggestion\",\n",
      "    \"instruction\": \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\",\n",
      "    \"instances\": [\n",
      "        {\n",
      "            \"input\": \"\",\n",
      "            \"output\": \"Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.\"\n",
      "        }\n",
      "    ],\n",
      "    \"is_classification\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Correct file path assignment (no trailing comma)\n",
    "seed_tasks_path = \"./seed_tasks.jsonl\"\n",
    "\n",
    "# Load the tasks from the file and parse each line as a JSON object\n",
    "seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
    "\n",
    "# Output the seed tasks\n",
    "print(json.dumps(seed_tasks[0],indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e14ddd3a-a74a-4ea7-8522-225d73289cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"instruction\": \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\",\n",
      "    \"input\": \"\",\n",
      "    \"output\": \"Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "seed_instruction_data = [\n",
    "    {\n",
    "        \"instruction\": task[\"instruction\"],\n",
    "        \"input\": task[\"instances\"][0][\"input\"],\n",
    "        \"output\": task[\"instances\"][0][\"output\"]\n",
    "    }\n",
    "    for task in seed_tasks\n",
    "]\n",
    "print(json.dumps(seed_instruction_data[0],indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3ec76a2a-d1fe-4661-84ca-beb120f12990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 machine-generated instructions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "\n",
    "output_dir = \"test_1\"\n",
    "\n",
    "\n",
    "def jload(filepath, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    # Check if the file exists and is not empty\n",
    "    if not os.path.exists(filepath) or os.stat(filepath).st_size == 0:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        with open(filepath, mode) as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from file {filepath}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize and load machine-generated instructions\n",
    "request_idx = 0\n",
    "machine_instruction_data = []\n",
    "\n",
    "regen_file_path = os.path.join(output_dir, \"regen.json\")\n",
    "if os.path.exists(regen_file_path):\n",
    "    machine_instruction_data = jload(regen_file_path)\n",
    "    print(f\"Loaded {len(machine_instruction_data)} machine-generated instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "115b0870-943e-4f7f-8a5e-9ca31c505fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\"\n",
      "Number of instruction generated 100\n",
      "Request batch size 5\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import tqdm\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "\n",
    "# first we tokenize all the seed instructions and generated machine instructions\n",
    "all_instructions = [d[\"instruction\"] for d in seed_instruction_data] + [d[\"instruction\"] for d in machine_instruction_data]\n",
    "\n",
    "all_instruction_tokens = [scorer._tokenizer.tokenize(inst) for inst in all_instructions]\n",
    "\n",
    "print(json.dumps(all_instructions[0], indent=4))\n",
    "print(f'Number of instruction generated {num_instructions_to_generate}')\n",
    "print(f'Request batch size {request_batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a0770d2-fb51-4892-9eb1-7098c08ca60d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 69\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError decoding JSON from file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopenai_completion\u001b[39m(prompts: Union[\u001b[38;5;28mstr\u001b[39m, Sequence[\u001b[38;5;28mstr\u001b[39m], Sequence[\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m], \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]], decoding_args: OpenAIDecodingArguments,\n\u001b[1;32m     70\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m, sleep_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_instances\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mmaxsize, max_batches\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mmaxsize, return_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoding_kwargs,\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Union[StrOrOpenAIObject], Sequence[StrOrOpenAIObject], Sequence[Sequence[StrOrOpenAIObject]],]:\n\u001b[1;32m     72\u001b[0m     is_single_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(prompts, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_single_prompt:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import dataclasses\n",
    "from typing import Optional, Sequence, Union\n",
    "from rouge_score import rouge_scorer\n",
    "import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class OpenAIDecodingArguments(object):\n",
    "    max_tokens: int = 1800\n",
    "    temperature: float = 0.2\n",
    "    top_p: float = 1.0\n",
    "    n: int = 1\n",
    "    stream: bool = False\n",
    "    stop: Optional[Sequence[str]] = None\n",
    "    presence_penalty: float = 0.0\n",
    "    frequency_penalty: float = 0.0\n",
    "    suffix: Optional[str] = None\n",
    "    logprobs: Optional[int] = None\n",
    "    echo: bool = False\n",
    "\n",
    "\n",
    "def encode_prompt(prompt_instructions):\n",
    "    \"\"\"Encode multiple prompt instructions into a single, more readable string.\"\"\"\n",
    "    # Read the initial prompt from a file\n",
    "    prompt = open(\"./prompt.txt\").read() + \"\\n\\n\"\n",
    "    \n",
    "    # Loop through the prompt instructions and format them\n",
    "    for idx, task_dict in enumerate(prompt_instructions):\n",
    "        instruction = task_dict[\"instruction\"]\n",
    "        input_text = task_dict[\"input\"]\n",
    "        output = task_dict[\"output\"]\n",
    "\n",
    "        # Clean up the instruction and input\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        input_text = \"<noinput>\" if input_text.strip() == \"\" else input_text.strip()\n",
    "\n",
    "        # Add formatted sections for instruction, input, and output\n",
    "        prompt += f\"### Task {idx + 1} ###\\n\"\n",
    "        prompt += f\"Instruction: {instruction}\\n\"\n",
    "        prompt += f\"Input:\\n{input_text}\\n\"\n",
    "        prompt += f\"Output:\\n{output}\\n\"\n",
    "        prompt += f\"#########################\\n\\n\"\n",
    "    \n",
    "    # Add the next instruction prompt for the model to complete\n",
    "    prompt += f\"### Task {len(prompt_instructions) + 1} ###\\n\"\n",
    "    prompt += f\"Instruction: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def jload(filepath, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    # Check if the file exists and is not empty\n",
    "    if not os.path.exists(filepath) or os.stat(filepath).st_size == 0:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        with open(filepath, mode) as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from file {filepath}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def openai_completion(prompts: Union[str, Sequence[str], Sequence[dict[str, str]], dict[str, str]], decoding_args: OpenAIDecodingArguments,\n",
    "    model_name=\"text-davinci-003\", sleep_time=2, batch_size=1, max_instances=sys.maxsize, max_batches=sys.maxsize, return_text=False, **decoding_kwargs,\n",
    ") -> Union[Union[StrOrOpenAIObject], Sequence[StrOrOpenAIObject], Sequence[Sequence[StrOrOpenAIObject]],]:\n",
    "    is_single_prompt = isinstance(prompts, (str, dict))\n",
    "    if is_single_prompt:\n",
    "        print(f'Prompt {prompts} is an instance of str or dict')\n",
    "        print(f'Transforming it into a list format to streamline processing')\n",
    "        prompts = [prompts]\n",
    "        print(f'Prompt {prompts} is an instance of str or dict')\n",
    "\n",
    "seed_tasks_path = \"./seed_tasks.jsonl\"\n",
    "output_dir = \"test_1\"\n",
    "num_instructions_to_generate=100\n",
    "request_batch_size=5\n",
    "request_idx = 0\n",
    "temperature=1.0\n",
    "top_p=1.0\n",
    "num_prompt_instructions=3\n",
    "\n",
    "# Correct file path assignment (no trailing comma)\n",
    "seed_tasks_path = \"./seed_tasks.jsonl\"\n",
    "\n",
    "# Load the tasks from the file and parse each line as a JSON object\n",
    "seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
    "\n",
    "seed_instruction_data = [\n",
    "    {\n",
    "        \"instruction\": task[\"instruction\"],\n",
    "        \"input\": task[\"instances\"][0][\"input\"],\n",
    "        \"output\": task[\"instances\"][0][\"output\"]\n",
    "    }\n",
    "    for task in seed_tasks\n",
    "]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize and load machine-generated instructions\n",
    "request_idx = 0\n",
    "machine_instruction_data = []\n",
    "\n",
    "regen_file_path = os.path.join(output_dir, \"regen.json\")\n",
    "if os.path.exists(regen_file_path):\n",
    "    machine_instruction_data = jload(regen_file_path)\n",
    "    print(f\"Loaded {len(machine_instruction_data)} machine-generated instructions\")\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "\n",
    "# first we tokenize all the seed instructions and generated machine instructions\n",
    "all_instructions = [d[\"instruction\"] for d in seed_instruction_data] + [d[\"instruction\"] for d in machine_instruction_data]\n",
    "\n",
    "all_instruction_tokens = [scorer._tokenizer.tokenize(inst) for inst in all_instructions]\n",
    "\n",
    "while len(machine_instruction_data) < num_instructions_to_generate:\n",
    "    request_idx += 1\n",
    "    batch_inputs = []\n",
    "    for _ in range(request_batch_size):\n",
    "        prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)\n",
    "        # print(f'### {num_prompt_instructions} random prompt from the seed instruction data \\n\\n\\n {json.dumps(prompt_instructions, indent=4)}')\n",
    "        prompt = encode_prompt(prompt_instructions)\n",
    "        # print(f'### {num_prompt_instructions} random prompt from the seed instruction data \\n\\n\\n {prompt}')\n",
    "        batch_inputs.append(prompt)\n",
    "        # break\n",
    "    print(f'### {request_batch_size} batch size and the following batch inputs \\n\\n\\n {prompt}')\n",
    "    decoding_args = OpenAIDecodingArguments(temperature=temperature, n=1, max_tokens=3072, top_p=top_p, stop=[\"\\n20\", \"20.\", \"20.\"])\n",
    "\n",
    "    request_start = time.time()\n",
    "    results = openai_completion(prompts=batch_inputs, model_name=model_name, batch_size=request_batch_size, decoding_args=decoding_args, logit_bias={\"50256\": -100})\n",
    "    request_duration = time.time() - request_start\n",
    "\n",
    "    print(f'### Open AI Auto-complete request duration {request_duration}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b73061-4e1f-48ce-b8bc-4f85d3130b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aa21e24c-7ac7-4754-ba08-2c66777a4123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are asked to come up with a set of 20 diverse task instructions. These task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\n",
      "\n",
      "Here are the requirements:\n",
      "1. Try not to repeat the verb for each instruction to maximize diversity.\n",
      "2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instrucitons.\n",
      "3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended generation, classification, editing, etc.\n",
      "2. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\n",
      "3. The instructions should be in English.\n",
      "4. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n",
      "5. You should generate an appropriate input to the instruction. The input field should contain a specific example provided for the instruction. It should involve realistic data and should not contain simple placeholders. The input should provide substantial content to make the instruction challenging but should ideally not exceed 100 words.\n",
      "6. Not all instructions require input. For example, when a instruction asks about some general information, \"what is the highest peak in the world\", it is not necssary to provide a specific context. In this case, we simply put \"<noinput>\" in the input field.\n",
      "7. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.\n",
      "\n",
      "List of 20 tasks:\n",
      "\n",
      "### Task 1 ###\n",
      "Instruction: Given the name of an exercise, explain how to do it.\n",
      "Input:\n",
      "Burpees\n",
      "Output:\n",
      "Starting with your feet shoulder-width apart, get down into a squat position. From a deep squat, put your hands down on the floor inside your feet and jump your feet back behind you, so you’re in a plank position, then do a push-up, touching your chest to the floor. Raise back up onto your arms in a plank, then jump your feet back in underneath your chest. Jump your feet outside your hands and as you stand up, jump up with your arms in the air. Aim for 10 reps.\n",
      "#########################\n",
      "\n",
      "### Task 2 ###\n",
      "Instruction: Are you smarter than most redheads?\n",
      "Input:\n",
      "<noinput>\n",
      "Output:\n",
      "That's a mean question! I try my best to be fair to everyone!\n",
      "#########################\n",
      "\n",
      "### Task 3 ###\n",
      "Instruction: Tell me if the following email is a promotion email.\n",
      "Input:\n",
      "Subject: A $3.50 transaction was made on your Citi Premier® account\n",
      "Sender: alerts@info6.citi.com\n",
      "Content:\n",
      "Hi, XXX. The transaction made on your Citi Premier® account exceeded $0.00. The transaction details are listed below.\n",
      "Amount: $3.50\n",
      "Card Ending In\n",
      "6211\n",
      "Merchant\n",
      "Barron Restaurant Seattle\n",
      "Date\n",
      "05/12/2021\n",
      "Time\n",
      "03:29 PM ET\n",
      "Output:\n",
      "no\n",
      "#########################\n",
      "\n",
      "### Task 4 ###\n",
      "Instruction: \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def encode_prompt(prompt_instructions):\n",
    "    \"\"\"Encode multiple prompt instructions into a single, more readable string.\"\"\"\n",
    "    # Read the initial prompt from a file\n",
    "    prompt = open(\"./prompt.txt\").read() + \"\\n\\n\"\n",
    "    \n",
    "    # Loop through the prompt instructions and format them\n",
    "    for idx, task_dict in enumerate(prompt_instructions):\n",
    "        instruction = task_dict[\"instruction\"]\n",
    "        input_text = task_dict[\"input\"]\n",
    "        output = task_dict[\"output\"]\n",
    "\n",
    "        # Clean up the instruction and input\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        input_text = \"<noinput>\" if input_text.strip() == \"\" else input_text.strip()\n",
    "\n",
    "        # Add formatted sections for instruction, input, and output\n",
    "        prompt += f\"### Task {idx + 1} ###\\n\"\n",
    "        prompt += f\"Instruction: {instruction}\\n\"\n",
    "        prompt += f\"Input:\\n{input_text}\\n\"\n",
    "        prompt += f\"Output:\\n{output}\\n\"\n",
    "        prompt += f\"#########################\\n\\n\"\n",
    "    \n",
    "    # Add the next instruction prompt for the model to complete\n",
    "    prompt += f\"### Task {len(prompt_instructions) + 1} ###\\n\"\n",
    "    prompt += f\"Instruction: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "prompt = encode_prompt(prompt_instructions)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dfd07409-cf81-4c22-9548-b3c5aae710ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m decoding_args \u001b[38;5;241m=\u001b[39m OpenAIDecodingArguments(\n\u001b[1;32m     23\u001b[0m             temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     24\u001b[0m             n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m             stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m20\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     28\u001b[0m         )\n\u001b[1;32m     31\u001b[0m request_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mopenai_completion(\n\u001b[1;32m     33\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mbatch_inputs,\n\u001b[1;32m     34\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     35\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mrequest_batch_size,\n\u001b[1;32m     36\u001b[0m     decoding_args\u001b[38;5;241m=\u001b[39mdecoding_args,\n\u001b[1;32m     37\u001b[0m     logit_bias\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50256\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m},  \u001b[38;5;66;03m# prevent the <|endoftext|> token from being generated\u001b[39;00m\n\u001b[1;32m     38\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "import time\n",
    "from typing import Optional, Sequence, Union\n",
    "\n",
    "temperature=1.0\n",
    "top_p=1.0\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class OpenAIDecodingArguments(object):\n",
    "    max_tokens: int = 1800\n",
    "    temperature: float = 0.2\n",
    "    top_p: float = 1.0\n",
    "    n: int = 1\n",
    "    stream: bool = False\n",
    "    stop: Optional[Sequence[str]] = None\n",
    "    presence_penalty: float = 0.0\n",
    "    frequency_penalty: float = 0.0\n",
    "    suffix: Optional[str] = None\n",
    "    logprobs: Optional[int] = None\n",
    "    echo: bool = False\n",
    "\n",
    "decoding_args = OpenAIDecodingArguments(\n",
    "            temperature=temperature,\n",
    "            n=1,\n",
    "            max_tokens=3072,  # hard-code to maximize the length. the requests will be automatically adjusted\n",
    "            top_p=top_p,\n",
    "            stop=[\"\\n20\", \"20.\", \"20.\"],\n",
    "        )\n",
    "\n",
    "\n",
    "request_start = time.time()\n",
    "results = utils.openai_completion(\n",
    "    prompts=batch_inputs,\n",
    "    model_name=model_name,\n",
    "    batch_size=request_batch_size,\n",
    "    decoding_args=decoding_args,\n",
    "    logit_bias={\"50256\": -100},  # prevent the <|endoftext|> token from being generated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437e6da-39bc-4123-a6af-e1fd03809035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43598f-c131-47a9-a9c3-765fe1ab8649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
